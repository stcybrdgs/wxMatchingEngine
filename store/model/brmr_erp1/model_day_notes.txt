train model vocab for ners
use the model for each breakout model
use the model to instantiate the tender nlp obj
pick up the hash
make a small nlp obj w/just the language model for each hash range
match stuff

come up with a strategy for determining partials when you have no hash

-----------------------------------------------------------------------
RULES-BASED MATCHING STUFF
-----------------------------------------------------------------------
Compared to using regular expressions on raw text, spaCy’s rule-based matcher engines and components not only let you find you the words and phrases you’re looking for – they also give you access to the tokens within the document and their relationships. This means you can easily:

- access and analyze the surrounding tokens
- merge spans into single tokens 
- add entries to the named entities in doc.ents.


SHOULD I USE RULES OR TRAIN A MODEL?
For complex tasks, it’s usually better to train a statistical entity recognition model. However, statistical models require training data, so for many situations, rule-based approaches are more practical. This is especially true at the start of a project: you can use a rule-based approach as part of a data collection process, to help you “bootstrap” a statistical model.

Rule-based systems are a good choice if there’s a more or less finite number of examples that you want to find in the data, or if there’s a very clear, structured pattern you can express with token rules or regular expressions. For instance, country names, IP addresses or URLs are things you might be able to handle well with a purely rule-based approach.


WHEN SHOULD I USE THE TOKEN MATCHER VS. THE PHRASE MATCHER?
The PhraseMatcher is useful if you already have a large terminology list or gazetteer consisting of single or multi-token phrases that you want to find exact instances of in your data. As of spaCy v2.1.0, you can also match on the LOWER attribute for fast and case-insensitive matching.

The Matcher isn’t as blazing fast as the PhraseMatcher, since it compares across individual token attributes. However, it allows you to write very abstract representations of the tokens you’re looking for, using lexical attributes, linguistic features predicted by the model, operators, set membership and rich comparison. For example, you can find a noun, followed by a verb with the lemma “love” or “like”, followed by an optional determiner and another token that’s at least ten characters long.


SO, 
- USE MATCHER FOR MATCHING BASED ON COMPLEX RULES
- USE PHRASEMATCHER FOR LARGE TERMINOLOGY LIST THAT YOU NEED TO FIND
  EXACT INSTANCES OF IN THE DATA

... AND AS OF SPACY V2.1.0, YOU CAN MATCH ON THE LOWER ATTRIBUTE FOR FAST AND CASE-INSENSITIVE MATCHING...

I'M GOING WITH RULES INSTEAD OF MODEL 
AND THE PHRASEMATCHER


-----------------------------------------------------------------------
MODEL TRAINING STUFF
-----------------------------------------------------------------------
training means feeding your model examples of text
so that it can predict POS, dependencies, and NERs

YOU CAN START WITH AN EXISTING, PRE-TRAINED MODEL (LIKE 'EN'), OR
YOU CAN TRAIN A MODEL FROM SCRATCH USING A BLANK LANGUAGE CLASS.

you want the model to be able to generalize across examples based on context

to see how well it is predicting, you need both training data and evaluation data

training a model from scratch takes hundreds of examples, but after that, it takes only a few examples to get decent results updating a pre-trained model

TRAIN VIA COMMAND LINE
For most purposes, the best way to train spaCy is via the command-line interface. The spacy train command takes care of many details for you, including making sure that the data is minibatched and shuffled correctly, progress is printed, and models are saved after each epoch. You can prepare your data for use in spacy train using the spacy convert command, which accepts many common NLP data formats, including .iob for named entities, and the CoNLL format for dependencies

TRAINING DATA
Collecting training data may sound incredibly painful – and it can be, if you’re planning a large-scale annotation project. However, if your main goal is to update an existing model’s predictions – for example, spaCy’s named entity recognition – the hard part is usually not creating the actual annotations. It’s finding representative examples and extracting potential candidates. 

SPACY TAKES TRAINING DATA IN JSON FORMAT

RULE-BASED MATCHER
Alternatively, the rule-based matcher can be a useful tool to extract tokens or combinations of tokens, as well as their start and end index in a document. 

entity : label

YOUR APP MAY HAVE ITS VERY OWN NER ANNOTATION SCHEME

====================================
BILUO Scheme
TAG	DESCRIPTION
BEGIN	The first token of a multi-token entity.
IN	An inner token of a multi-token entity.
LAST	The final token of a multi-token entity.
UNIT	A single-token entity.
OUT	A non-entity token.

WHY BILUO, NOT IOB?
There are several coding schemes for encoding entity annotations as token tags. These coding schemes are equally expressive, but not necessarily equally learnable. Ratinov and Roth showed that the minimal Begin, In, Out scheme was more difficult to learn than the BILUO scheme that we use, which explicitly marks boundary tokens.


====================================
PRODIGY
If you need to label a lot of data, check out Prodigy, a new, active learning-powered annotation tool we’ve developed. Prodigy is fast and extensible, and comes with a modern web application that helps you collect training data faster. It integrates seamlessly with spaCy, pre-selects the most relevant examples for annotation, and lets you train and evaluate ready-to-use spaCy models.

spaCy translates the character offsets into this scheme, in order to decide the cost of each action given the current state of the entity recogniser. The costs are then used to calculate the gradient of the loss, to train the model. The exact algorithm is a pastiche of well-known methods, and is not currently described in any single publication. The model is a greedy transition-based parser guided by a linear model whose weights are learned using the averaged perceptron loss, via the dynamic oracle imitation learning strategy. The transition system is equivalent to the BILUO tagging scheme.


=======================================
train_data = [
    ("Uber blew through $1 million a week", [(0, 4, 'ORG')]),
    ("Android Pay expands to Canada", [(0, 11, 'PRODUCT'), (23, 30, 'GPE')]),
    ("Spotify steps up Asia expansion", [(0, 8, "ORG"), (17, 21, "LOC")]),
    ("Google Maps launches location sharing", [(0, 11, "PRODUCT")]),
    ("Google rebrands its business apps", [(0, 6, "ORG")]),
    ("look what i found on google!", [(21, 27, "PRODUCT")])
]

train_data = [
    ("01-00-00-00 IS A DEEP GROOVE BALL BEARING", [(17, 41, 'PRODUCT')]),
    ("01-01-12-03 IS A CYLINDRICAL NEEDLE BEARING FROM SKF", [(0, 11, 'HASH'), (17, 43, 'PRODUCT'), (49, 52, 'SUPPLIER')]),
]


========================================
TRAINING WITH ANNOTATINOS	
The GoldParse object collects the annotated training examples, also called the gold standard. It’s initialized with the Doc object it refers to, and keyword arguments specifying the annotations, like tags or entities. Its job is to encode the annotations, keep them aligned and create the C-level data structures required for efficient access. Here’s an example of a simple GoldParse for part-of-speech tags:

vocab = Vocab(tag_map={"N": {"pos": "NOUN"}, "V": {"pos": "VERB"}})
doc = Doc(vocab, words=["I", "like", "stuff"])
gold = GoldParse(doc, tags=["N", "V", "N"])

Using the Doc and its gold-standard annotations, the model can be updated to learn a sentence of three words with their assigned part-of-speech tags. The tag map is part of the vocabulary and defines the annotation scheme. If you’re training a new language model, this will let you map the tags present in the treebank you train on to spaCy’s tag scheme.

doc = Doc(Vocab(), words=["Facebook", "released", "React", "in", "2014"])
gold = GoldParse(doc, entities=["U-ORG", "O", "U-TECHNOLOGY", "O", "U-DATE"])

The same goes for named entities. The letters added before the labels refer to the tags of the BILUO scheme – O is a token outside an entity, U an single entity unit, B the beginning of an entity, I a token inside an entity and L the last token of an entity.


=========================================
OPTIONS
- use Prodigy
- use sequences of Doc and GoldParse objects
    - write your own training loop (w/ json file or raw texts)
    - use command line with 'train' and 'convert' (w/ json file or raw texts)
- use 'simple training style' (packaged with v 2.0)


SIMPLE TRAINING STYLE  -----------------------------------

Instead of sequences of Doc and GoldParse objects, you can also use the “simple training style” and pass raw texts and dictionaries of annotations to nlp.update. The dictionaries can have the keys entities, heads, deps, tags and cats. This is generally recommended, as it removes one layer of abstraction, and avoids unnecessary imports. It also makes it easier to structure and load your training data.
	
TRAIN_DATA = [
        (u"Uber blew through $1 million a week", {"entities": [(0, 4, "ORG")]}),
        (u"Google rebrands its business apps", {"entities": [(0, 6, "ORG")]})]

nlp = spacy.blank('en')
optimizer = nlp.begin_training()
for i in range(20):
    random.shuffle(TRAIN_DATA)
    for text, annotations in TRAIN_DATA:
        nlp.update([text], [annotations], sgd=optimizer)
nlp.to_disk("/model")

The above training loop leaves out a few details that can really improve accuracy – but the principle really is that simple. Once you’ve got your pipeline together and you want to tune the accuracy, you usually want to process your training examples in batches, and experiment with MINIBATCH sizes and dropout rates, set via the DROP keyword argument. See the LANGUAGE and PIPE API docs for available options.


=================================================
UPDATING THE NAMED ENTITY RECOGNIZER

you can update the NER with you own examples, starting from either a pre-trained model or a blank one.
To do this, you need:
- example texts
- character offsets for each entity in the texts
- labels for each entity in the texts

STEP-BY-STEP GUIDE
Rem you are writing a training module
1. LOAD THE MODEL you want to start with (like usual)
    - if it's blank, remember to add the Entity Recognizer to the pipeline 
    - if it is a pre-existing model, remember to DISABLE ALL OTHER PIPELINE COMPONENTS,
      so that you are only training the ENTITY RECOGNIZER
2. SHUFFLE & LOOP over the examples (ie, write a training loop)
    - on each loop, call nlp.update to step through the words of the input
      at each word, it makes a prediction
      then consults the annotations to see if it was right
      if it was wrong it adjusts its weights so that the correct action will score higher next time
3. SAVE THE TRAINED MODEL using nlp.to_disk
4. TEST THE MODEL to make sure the entities in the training data are recognized correctly

TRAIN SO THE MODEL DOES NOT FORGET
If you’re using an existing model, make sure to mix in examples of other entity types that spaCy correctly recognized before. Otherwise, your model might learn the new type, but “forget” what it previously knew. This is also referred to as the “catastrophic forgetting” problem.


=============================================
SAVING AND LOADING
If you’ve been modifying the pipeline, vocabulary, vectors and entities, or made updates to the model, you’ll eventually want to save your progress – for example, everything that’s in your nlp object. This means you’ll have to translate its contents and structure into a format that can be saved, like a file or a byte string. This process is called serialization. spaCy comes with built-in serialization methods and supports the Pickle protocol.

Pickle is Python’s built-in object persistence system. It lets you transfer arbitrary Python objects between processes. This is usually used to load an object to and from disk, but it’s also used for distributed computing, e.g. with PySpark or Dask. When you unpickle an object, you’re agreeing to execute whatever code it contains. It’s like calling eval() on a string – so don’t unpickle objects from untrusted sources.

All container classes, i.e. Language (nlp), Doc, Vocab and StringStore have the following methods available:

METHOD		RETURNS	EXAMPLE
to_bytes		bytes		data = nlp.to_bytes()
from_bytes	object		nlp.from_bytes(data)
to_disk		-		nlp.to_disk("/path")
from_disk		object		nlp.from_disk("/path")

===============================================
SERIALIZING THE PIPELINE
When serializing the pipeline, keep in mind that this will only save out the binary data for the individual components to allow spaCy to restore them – not the entire objects. This is a good thing, because it makes serialization safe. But it also means that you have to take care of storing the language name and pipeline component names as well, and restoring them separately before you can load in the data.

SERIALIZE:
bytes_data = nlp.to_bytes()
lang = nlp.meta["lang"]  # "en"
pipeline = nlp.meta["pipeline"]  # ["tagger", "parser", "ner"]

DESERIALIZE:
nlp = spacy.blank(lang)
for pipe_name in pipeline:
    pipe = nlp.create_pipe(pipe_name)
    nlp.add_pipe(pipe)
nlp.from_bytes(bytes_data)

This is also how spaCy does it under the hood when loading a model: it loads the model’s meta.json containing the language and pipeline information, initializes the language class, creates and adds the pipeline components and then loads in the binary data. 

PICKLING OBJECTS WITH SHARED DATA
When pickling spaCy’s objects like the Doc or the EntityRecognizer, keep in mind that they all require the shared Vocab (which includes the string to hash mappings, label schemes and optional vectors). This means that their pickled representations can become very large, especially if you have word vectors loaded, because it won’t only include the object itself, but also the entire shared vocab it depends on.

If you need to pickle multiple objects, try to pickle them together instead of separately. For instance, instead of pickling all pipeline components, pickle the entire pipeline once. And instead of pickling several Doc objects separately, pickle a list of Doc objects. Since the all share a reference to the same Vocab object, it will only be included once.

# pickling objects with shared data:
doc1 = nlp(u"Hello world")
doc2 = nlp(u"This is a test")

doc1_data = pickle.dumps(doc1)
doc2_data = pickle.dumps(doc2)
print(len(doc1_data) + len(doc2_data))  # 6636116 😞

doc_data = pickle.dumps([doc1, doc2])


FOR SCRIPTS TO PICKLE/UNPICKLE:
https://spacy.io/usage/saving-loading
print(len(doc_data)) 
